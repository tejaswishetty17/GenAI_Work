{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyuetRB4Y/1Dv3eaB6pmti",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejaswishetty17/GenAI_Work/blob/main/Adding_Meta_Data_Along_with_the_response.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-hk2ED7nW3b",
        "outputId": "34e54c7d-f9d9-48a4-edba-40725764e1bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "pip install -q langchain langchain-openai langchain-community chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from operator import itemgetter"
      ],
      "metadata": {
        "id": "ojYsosdDne7O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"<YOUR_API_KEY>\"\n"
      ],
      "metadata": {
        "id": "_zlIJIrfntfo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Load and split document\n",
        "loader = TextLoader(\"/content/sample_data/langchain_doc.txt\")\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "irzNX_CCnvFP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Embedding and vector store\n",
        "embedding = OpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
        "vectorstore = Chroma.from_documents(chunks, embedding=embedding, persist_directory=\"./chroma_rag_simple1\")\n",
        "\n",
        "retriver = vectorstore.as_retriever(serach_kwargs={\"k\":3})"
      ],
      "metadata": {
        "id": "5cHZkqCMoLON"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriver.invoke(\"what is langchain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV7L_6M-pPCe",
        "outputId": "e4ee6262-e109-48c0-de28-ccdde115e1c4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/sample_data/langchain_doc.txt'}, page_content='LangChain offers features and tools that make it easier to work with this concept. In particular, the use of modular abstractions, prebuilt utilities, and community-backed best practices ensures that developers can implement solutions around langchain for data extraction and transformation efficiently. Whether integrating third-party APIs, working with documents, or building agents, LangChain provides end-to-end support for langchain for data extraction and transformation.'),\n",
              " Document(metadata={'source': '/content/sample_data/langchain_doc.txt'}, page_content='LangChain offers features and tools that make it easier to work with this concept. In particular, the use of modular abstractions, prebuilt utilities, and community-backed best practices ensures that developers can implement solutions around langchain for data extraction and transformation efficiently. Whether integrating third-party APIs, working with documents, or building agents, LangChain provides end-to-end support for langchain for data extraction and transformation.'),\n",
              " Document(metadata={'source': '/content/sample_data/langchain_doc.txt'}, page_content='LangChain offers features and tools that make it easier to work with this concept. In particular, the use of modular abstractions, prebuilt utilities, and community-backed best practices ensures that developers can implement solutions around langchain introduction and core philosophy efficiently. Whether integrating third-party APIs, working with documents, or building agents, LangChain provides end-to-end support for langchain introduction and core philosophy.'),\n",
              " Document(metadata={'source': '/content/sample_data/langchain_doc.txt'}, page_content='LangChain offers features and tools that make it easier to work with this concept. In particular, the use of modular abstractions, prebuilt utilities, and community-backed best practices ensures that developers can implement solutions around langchain introduction and core philosophy efficiently. Whether integrating third-party APIs, working with documents, or building agents, LangChain provides end-to-end support for langchain introduction and core philosophy.')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Format documents with source metadata\n",
        "\n",
        "def format_docs(docs):\n",
        "  context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "  sources = list(set(doc.metadata.get(\"source\", \"Unknown\") for doc in docs))\n",
        "  return context, sources"
      ],
      "metadata": {
        "id": "5fMhYxwnoy0p"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Define the prompt and LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a helful assistant. Use the below context below to answer the question:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:{question}\n",
        "\"\"\")\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "t9VssKC7pbnW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5: RAG Flow\n",
        "\n",
        "query = \"What is Langchain used for?\"\n",
        "\n",
        "#Get relavant documents\n",
        "retrieved_docs = retriver.invoke(query)\n",
        "\n",
        "#Format them\n",
        "context, sources = format_docs(retrieved_docs)\n",
        "\n",
        "print(\"Context: \",context)\n",
        "print(\"Sources: \",sources, \"\\n\\n\")\n",
        "\n",
        "#Run through the LLM\n",
        "answer = chain.invoke({\"context\":context, \"question\":query})\n",
        "\n",
        "#Step 6: Print Results\n",
        "print(\"Question\", query)\n",
        "print(\"Answer\", answer)\n",
        "print(\"Sources:\", sources)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM69SQ1Yp_TP",
        "outputId": "1c298f61-8785-469a-9f7b-d9d6d57793d2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:  LangChain offers features and tools that make it easier to work with this concept. In particular, the use of modular abstractions, prebuilt utilities, and community-backed best practices ensures that developers can implement solutions around advanced use cases: coding assistants and research bots efficiently. Whether integrating third-party APIs, working with documents, or building agents, LangChain provides end-to-end support for advanced use cases: coding assistants and research bots.\n",
            "\n",
            "LangChain offers features and tools that make it easier to work with this concept. In particular, the use of modular abstractions, prebuilt utilities, and community-backed best practices ensures that developers can implement solutions around advanced use cases: coding assistants and research bots efficiently. Whether integrating third-party APIs, working with documents, or building agents, LangChain provides end-to-end support for advanced use cases: coding assistants and research bots.\n",
            "\n",
            "LangChain offers features and tools that make it easier to work with this concept. In particular, the use of modular abstractions, prebuilt utilities, and community-backed best practices ensures that developers can implement solutions around multi-modal applications using langchain efficiently. Whether integrating third-party APIs, working with documents, or building agents, LangChain provides end-to-end support for multi-modal applications using langchain.\n",
            "\n",
            "LangChain offers features and tools that make it easier to work with this concept. In particular, the use of modular abstractions, prebuilt utilities, and community-backed best practices ensures that developers can implement solutions around multi-modal applications using langchain efficiently. Whether integrating third-party APIs, working with documents, or building agents, LangChain provides end-to-end support for multi-modal applications using langchain.\n",
            "Sources:  ['/content/sample_data/langchain_doc.txt'] \n",
            "\n",
            "\n",
            "Question What is Langchain used for?\n",
            "Answer LangChain is used for developing advanced applications such as coding assistants and research bots. It provides features and tools that facilitate the implementation of solutions around these use cases, including modular abstractions, prebuilt utilities, and community-backed best practices. Additionally, LangChain supports multi-modal applications, enabling developers to integrate third-party APIs, work with documents, and build agents efficiently. Overall, it offers end-to-end support for various advanced use cases in application development.\n",
            "Sources: ['/content/sample_data/langchain_doc.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lets do handle data from multiple files"
      ],
      "metadata": {
        "id": "K1pJezyBrfKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Install required packages (if not already done)\n",
        "!pip install -q langchain langchain-openai langchain-community chromadb\n",
        "\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "file_path = [\n",
        "    \"/content/sample_data/langchain_doc.txt\",\n",
        "    \"/content/sample_data/spark_langchain.txt\"\n",
        "]\n",
        "\n",
        "\n",
        "docs = []\n",
        "\n",
        "for path in file_path:\n",
        "  loader = TextLoader(path)\n",
        "  docs.extend(loader.load())\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 50)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "#step 2: Embedding and Vectore Store\n",
        "embedding = OpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
        "vectorestore = Chroma.from_documents(chunks, embedding, persist_directory=\"./chroma_rag_simple3\")\n",
        "retriever = vectorestore.as_retriever(serach_kwargs = {\"k\":3})\n",
        "\n",
        "#Step 3: Format documents with source metadata\n",
        "def format_docs(docs):\n",
        "  context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "  sources = list(set(doc.metadata.get(\"source\", \"Unknown\") for doc in docs))\n",
        "  return context, sources\n",
        "\n",
        "#Step 4: Define Prompt and LLM\n",
        "llm = ChatOpenAI(model = \"gpt-4o-mini\", temperature=0)\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a helpful assistant. Use the context below to answer the question:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\")\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "cShajiGPqyCO"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: RAG Flow\n",
        "query = \"What is Langchain\"\n",
        "\n",
        "#Get relavent documents\n",
        "retrieved_docs = retriever.invoke(query)\n",
        "\n",
        "#Format them\n",
        "context, sources = format_docs(retrieved_docs)\n",
        "\n",
        "print(\"Retrieved Context: \", context)\n",
        "print(\"Retrived sources: \", sources, \"\\n\\n\\n\")\n",
        "\n",
        "#Run through the LLM\n",
        "answer = chain.invoke({\"context\": context, \"question\": query})\n",
        "\n",
        "#Step 6:\n",
        "print(\"Question:\", query)\n",
        "print(\"Answer:\", answer)\n",
        "print(\"Sources:\", sources)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm2f4sbzuFYl",
        "outputId": "cfca9561-e20b-49c0-afab-02fc05d45cff"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved Context:  What is LangChain?\n",
            "\n",
            "LangChain is a framework for developing applications powered by large language models (LLMs) like GPT.\n",
            "\n",
            "What is LangChain?\n",
            "\n",
            "LangChain is a framework for developing applications powered by large language models (LLMs) like GPT.\n",
            "\n",
            "What is LangChain?\n",
            "\n",
            "LangChain is a framework for developing applications powered by large language models (LLMs) like GPT.\n",
            "\n",
            "What is LangChain?\n",
            "\n",
            "LangChain is a framework for developing applications powered by large language models (LLMs) like GPT.\n",
            "Retrived sources:  ['/content/sample_data/spark_langchain.txt'] \n",
            "\n",
            "\n",
            "\n",
            "Question: What is Langchain\n",
            "Answer: LangChain is a framework for developing applications powered by large language models (LLMs) like GPT.\n",
            "Sources: ['/content/sample_data/spark_langchain.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zwxIKIxVu89V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}