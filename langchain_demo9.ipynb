{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl/XzdE2UTHAyFOEfHfHI+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejaswishetty17/GenAI_Work/blob/main/langchain_demo9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Runnable Prallel"
      ],
      "metadata": {
        "id": "Ketm4yOhKVYD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ORVpMJRsUsn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531f2eb4-7b19-4e2b-8bf3-1055b1b9e949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/62.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/437.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m430.1/437.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m437.7/437.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q langchain-openai langchain-core langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda, RunnableSequence, RunnableParallel, RunnableBranch\n",
        "import os"
      ],
      "metadata": {
        "id": "0UqFBxybU_IC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"<API_KEY>\"\n"
      ],
      "metadata": {
        "id": "8Wz-qfy0VP6o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model = ChatOpenAI(\n",
        "    model_name = \"gpt-4o-mini\",\n",
        "    temperature=.7\n",
        ")"
      ],
      "metadata": {
        "id": "XfYxL8dLox02"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(text:str) -> str:\n",
        "  \"\"\"Perform sentiment analysis on the input text.\"\"\"\n",
        "  prompt = f\"Analyze the sentiment of the following text:{text}\"\n",
        "  return chat_model.invoke(prompt).content\n",
        "\n",
        "def extract_entities(text:str)-> str:\n",
        "  \"\"\"Extract named entities (like names, places and organization) from text.\"\"\"\n",
        "  prompt = f\"Extract named entities from this text:{text}\"\n",
        "  return chat_model.invoke(prompt).content\n",
        "\n",
        "def extract_keywords(text:str)-> str:\n",
        "  \"\"\"Extract import keywords from the given text.\"\"\"\n",
        "  prompt = f\"Extract import keywords from this text: {text}\"\n",
        "  return chat_model.invoke(prompt).content"
      ],
      "metadata": {
        "id": "l8MYAq-8Dj6q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_tasks = RunnableParallel(\n",
        "    sentiment = RunnableLambda(analyze_sentiment),\n",
        "    entities = RunnableLambda(extract_entities),\n",
        "    keywords = RunnableLambda(extract_keywords)\n",
        ")"
      ],
      "metadata": {
        "id": "VM9vWxEDFc9S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Formatting function to structure the output\n",
        "format_results = RunnableLambda(lambda results:f\"\"\"\n",
        "ğŸ“Š**Sentiment Analysis**:{results['sentiment']}\n",
        "ğŸ”**Named Entities**:{results['entities']}\n",
        "ğŸ”‘**Keywords**:{results['keywords']}\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "vZM3kyVeF15B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = parallel_tasks | format_results"
      ],
      "metadata": {
        "id": "L2nb8jE9G2n8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Apple is planning to open a new office in San Francisco, creating many new job opportunities.\""
      ],
      "metadata": {
        "id": "joHmiFc2G-Yc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke(input_text)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CguFLudLHLXv",
        "outputId": "550b5ce2-7e7d-4abe-e24b-730bbb3a6216"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š**Sentiment Analysis**:The sentiment of the text is positive. It highlights a favorable developmentâ€”Apple opening a new officeâ€”which is expected to create many new job opportunities. This suggests growth and economic improvement, which generally evokes positive feelings.\n",
            "ğŸ”**Named Entities**:Named entities extracted from the text:\n",
            "\n",
            "1. Apple (Organization)\n",
            "2. San Francisco (Location)\n",
            "ğŸ”‘**Keywords**:- Apple\n",
            "- new office\n",
            "- San Francisco\n",
            "- job opportunities        \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example Use-case : Prompt Templates | Chaining"
      ],
      "metadata": {
        "id": "LB6dQxfLKOL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classification_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content = \"\"\"You are an AI that classifies text into 'question', 'story', 'statement'.\n",
        "    only return one of these three words.\"\"\"),\n",
        "    (\"human\", \"Classify the following text:\\n\\n{text} \\n Category:\")\n",
        "])"
      ],
      "metadata": {
        "id": "uE5nwno8MVFy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_step = classification_prompt | chat_model | (lambda x: {\"category\": x.content.strip().lower()})"
      ],
      "metadata": {
        "id": "HNVdTWJWNCwv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content = \"\"\"You are an AI answers question concisely.\"\"\"),\n",
        "    (\"human\", \"Answer this question:{text}\")\n",
        "])"
      ],
      "metadata": {
        "id": "VOOn7-v7PsMP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content = \"\"\"You are an AI that answers summarizes stories concisely.\"\"\"),\n",
        "    (\"human\", \"Summarize this story:{text}\")\n",
        "])"
      ],
      "metadata": {
        "id": "7UGKgHfOQGmE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content = \"\"\"You are an AI that generates creative and engaging titles for statements.\"\"\"),\n",
        "    (\"human\", \"create a short, engaging title for the following statement:{text}\")\n",
        "])"
      ],
      "metadata": {
        "id": "iNJ4Mi2oQSoP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question = answer_prompt | chat_model | (lambda x: {\"response\": x.content.strip()})"
      ],
      "metadata": {
        "id": "qa_2VTbvQ8LO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_story = summary_prompt | chat_model | (lambda x: {\"response\": x.content.strip()})"
      ],
      "metadata": {
        "id": "QsCf83EmRMYl"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_title = title_prompt | chat_model | (lambda x: {\"response\": x.content.strip()})"
      ],
      "metadata": {
        "id": "VcXJPKo4RNMO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = (\n",
        "    RunnableLambda(lambda x: {\"text\" : x}) |\n",
        "    (lambda x : {**x, **classification_step.invoke(x)}) |\n",
        "    RunnableBranch(\n",
        "        (lambda x: x[\"category\"] == \"question\", answer_question),\n",
        "        (lambda x: x[\"category\"] == \"story\", summarize_story),\n",
        "        (generate_title)\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "fexiCroQMPZe"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [\n",
        "    \"What is the capital of Japan?\",\n",
        "    \"Once upon a time in a faraway land, there was a brave knight who fought dragons to protect his kingdom.\",\n",
        "    \"Technology is evolving rapidly, and AI is shaping the future of work.\",\n",
        "    \"Self-driving cars will change the way we travel.\",\n",
        "]"
      ],
      "metadata": {
        "id": "j-NvfxShKa4q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_text in inputs:\n",
        "  response = pipeline.invoke(input_text)\n",
        "  print(f\"ğŸ–Šï¸ **Input** : {input_text} \\n ğŸ”Š **Responses {response['response']}\\n {'-'*50}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg8XPv6rMCKm",
        "outputId": "4cc7b442-7339-408e-a5b1-745168c7d2d6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ–Šï¸ **Input** : What is the capital of Japan? \n",
            " ğŸ”Š **Responses The capital of Japan is Tokyo.\n",
            " --------------------------------------------------\n",
            "ğŸ–Šï¸ **Input** : Once upon a time in a faraway land, there was a brave knight who fought dragons to protect his kingdom. \n",
            " ğŸ”Š **Responses In a distant land, a courageous knight battled dragons to safeguard his kingdom.\n",
            " --------------------------------------------------\n",
            "ğŸ–Šï¸ **Input** : Technology is evolving rapidly, and AI is shaping the future of work. \n",
            " ğŸ”Š **Responses \"AI: The Catalyst for Tomorrow's Workplace Revolution\"\n",
            " --------------------------------------------------\n",
            "ğŸ–Šï¸ **Input** : Self-driving cars will change the way we travel. \n",
            " ğŸ”Š **Responses \"Revolution on Wheels: The Future of Travel with Self-Driving Cars\"\n",
            " --------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}